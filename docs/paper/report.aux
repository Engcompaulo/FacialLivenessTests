\relax 
\citation{NPRArticle}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{report.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {paragraph}{Context}{1}}
\@writefile{toc}{\contentsline {paragraph}{Aims}{1}}
\@writefile{toc}{\contentsline {paragraph}{Method}{1}}
\@writefile{toc}{\contentsline {paragraph}{Results}{1}}
\@writefile{toc}{\contentsline {paragraph}{Conclusions}{1}}
\citation{FaceSpoofingAttacksStudy}
\citation{FaceSpoofingAttacksStudy}
\citation{SFMClassifier}
\citation{SFMClassifier}
\citation{BlinkDetectionLivenessTest}
\citation{ImageQualityAssessmentTest}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}2D Spoofing Attacks}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1}Video-based tests}{2}}
\@writefile{toc}{\contentsline {paragraph}{Depth-based liveness}{2}}
\@writefile{toc}{\contentsline {paragraph}{Blink Detection}{2}}
\@writefile{toc}{\contentsline {paragraph}{Face Flashing}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2}Quality-based tests}{2}}
\citation{Patel2016CrossDatabaseFA}
\citation{AlexNet}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{GoogLeNet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3}Deep Learning Liveness Tests}{3}}
\@writefile{tdo}{\contentsline {todo}{Fill in.}{3}}
\pgfsyspdfmark {pgfid1}{3818850}{50870829}
\pgfsyspdfmark {pgfid4}{38844054}{50868536}
\pgfsyspdfmark {pgfid5}{40695445}{50649646}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.4}Neural Networks, and their structure}{3}}
\@writefile{toc}{\contentsline {paragraph}{Fully connected layers}{3}}
\@writefile{toc}{\contentsline {paragraph}{Convolutional layers}{3}}
\@writefile{toc}{\contentsline {paragraph}{A note about parameters}{3}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Database Approach}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.5}General 2D image classification models}{3}}
\@writefile{toc}{\contentsline {paragraph}{AlexNet}{3}}
\@writefile{toc}{\contentsline {paragraph}{VGG16 Network}{3}}
\@writefile{toc}{\contentsline {paragraph}{GoogLeNet Inception}{3}}
\citation{DeepResidualNetworks}
\citation{DeepResidualNetworks}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{NUAADataset}
\citation{ReplayAttackDataset}
\citation{FaceSpoofingAttacksStudy}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A Residual Block: this is the building block of the ResNet architecture. Source: \cite  {DeepResidualNetworks}\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ResidualBlock}{{1}{4}}
\newlabel{Top1AccuracyOverNetwork}{{2a}{4}}
\newlabel{sub@Top1AccuracyOverNetwork}{{a}{4}}
\newlabel{Top1AccuracyOverOperations}{{2b}{4}}
\newlabel{sub@Top1AccuracyOverOperations}{{b}{4}}
\newlabel{ComparisonsOfDifferentModels}{{\caption@xref {ComparisonsOfDifferentModels}{ on input line 246}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces These two charts, from \cite  {DeepNeuralNetworkDeployability}, show the accuracy and computational requirements of each type of classifier.\relax }}{4}}
\@writefile{toc}{\contentsline {paragraph}{Residual Networks}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.6}Datasets}{4}}
\citation{3DMadDataset}
\citation{SFMClassifier}
\citation{3DReconstructionMethod}
\citation{VRNTorchToKeras}
\citation{VRNTorchToKeras}
\citation{PointNet}
\citation{VoxNetModel}
\citation{VoxNetModel}
\citation{VoxNetModel}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A screenshot from the VRN Torch to Keras version (a conversion of the VRN model to Keras). This screenshot is from \cite  {VRNTorchToKeras}\relax }}{5}}
\newlabel{3DReconstructionScreenshot}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}3D Spoofing Attacks}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1}Obtaining 3D data from 2D images/video}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2}3D Classifiers}{5}}
\@writefile{toc}{\contentsline {paragraph}{PointNet}{5}}
\@writefile{toc}{\contentsline {paragraph}{VoxNet}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The original VoxNet architecture. While point cloud input is available, voxel based representation can also be used. Diagram obtained from \cite  {VoxNetModel}\relax }}{6}}
\newlabel{OriginalVoxNetArchitecture}{{4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Solution}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Specification, and design requirements}{6}}
\newlabel{SpecPoint1}{{1}{6}}
\newlabel{SpecPoint2}{{2}{6}}
\newlabel{SpecPoint3}{{3}{6}}
\newlabel{SpecPoint4}{{4}{6}}
\newlabel{SpecPoint5}{{5}{6}}
\newlabel{SpecPoint6}{{6}{6}}
\newlabel{SpecPoint7}{{7}{6}}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\citation{VideoQualityOriginal}
\citation{VideoQualityUpdated}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Shared Services}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1}Dataset Managers}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2}Neural Network Infrastructure}{7}}
\@writefile{toc}{\contentsline {paragraph}{Neural Network Framework}{7}}
\@writefile{toc}{\contentsline {paragraph}{Hardware for training}{7}}
\@writefile{toc}{\contentsline {paragraph}{Hardware for testing}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3}Image Processing and Computation Management}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Image Quality Assessment Liveness Test}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1}Overview}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2}The Metrics}{7}}
\citation{BIQIImplementation}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The architecture of the image quality liveness test. (A) The greyscale copy of the image, and a blurred copy of the image are input into each of the metrics. (B) The metrics are individually calculated, and a single value output from them. (C) These values build a 1D vector. (D) They are classified using an LDA classifier. (E) The realness value is 1.0 for real, and 0.0 for fake, or in between.\relax }}{8}}
\newlabel{ImageQualityLivenessTestDiagram}{{5}{8}}
\@writefile{toc}{\contentsline {paragraph}{PyVideoQuality Library}{8}}
\@writefile{toc}{\contentsline {paragraph}{Absolute Difference (AD)}{8}}
\@writefile{toc}{\contentsline {paragraph}{Blind Image Quality Index (BIQI)}{8}}
\citation{JQIPaper}
\citation{LMSEPaper}
\citation{LMSEPaper}
\@writefile{toc}{\contentsline {paragraph}{Gradient Magnitude Error (GME)}{9}}
\@writefile{tdo}{\contentsline {todo}{look into this, because I think GME and GPE might have been switched during implementation for some reason.}{9}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Phase Error (GPE)}{9}}
\pgfsyspdfmark {pgfid6}{12172401}{47366303}
\pgfsyspdfmark {pgfid9}{38844054}{47364010}
\pgfsyspdfmark {pgfid10}{40695445}{47145120}
\@writefile{toc}{\contentsline {paragraph}{High Low Frequency Index (HLFI)}{9}}
\@writefile{toc}{\contentsline {paragraph}{JPEG Quality Index (JQI)}{9}}
\@writefile{tdo}{\contentsline {todo}{EXPLAIN MORE}{9}}
\pgfsyspdfmark {pgfid11}{27097247}{29344514}
\pgfsyspdfmark {pgfid14}{38844054}{29342221}
\pgfsyspdfmark {pgfid15}{40695445}{29123331}
\@writefile{tdo}{\contentsline {todo}{EXPLAIN MORE WHAT THESE DO}{9}}
\pgfsyspdfmark {pgfid16}{20649729}{27850298}
\pgfsyspdfmark {pgfid19}{38844054}{27315859}
\pgfsyspdfmark {pgfid20}{40695445}{27096969}
\@writefile{toc}{\contentsline {paragraph}{Laplacian Mean Squared Metric (LMSE)}{9}}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\@writefile{toc}{\contentsline {paragraph}{Mean Angle Magnitude Similarity (MAMS)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Mean Angle Similarity (MAS)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Maximum Difference (MD)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Mean Squared Error (MSE)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Normalized Absolute Error (NAE)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Naturalness Estimator (NIQE)}{10}}
\@writefile{tdo}{\contentsline {todo}{Explain the library implementation and roughly how it works.}{10}}
\pgfsyspdfmark {pgfid21}{20485939}{25957254}
\pgfsyspdfmark {pgfid24}{38844054}{25954961}
\pgfsyspdfmark {pgfid25}{40695445}{25736071}
\@writefile{toc}{\contentsline {paragraph}{Normalized Cross Correlation (NXC)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Peak Signal to Noise Ratio (PSNR)}{10}}
\@writefile{toc}{\contentsline {paragraph}{R-Averaged Metric (RAMD)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Structural Content (SC)}{10}}
\citation{SSIMPaper}
\@writefile{toc}{\contentsline {paragraph}{Spectral Magnitude Error (SME)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Signal to Noise Ratio (SNR)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Spectral Phase Error (SPE)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Structural Similarity (SIM)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Total Corner Difference (TCD)}{11}}
\@writefile{tdo}{\contentsline {todo}{More about how we used Corner Harris to make it work (because this function is weird)}{11}}
\pgfsyspdfmark {pgfid26}{4474211}{16830512}
\pgfsyspdfmark {pgfid29}{38844054}{16828219}
\pgfsyspdfmark {pgfid30}{40695445}{16609329}
\@writefile{toc}{\contentsline {paragraph}{Total Edge Difference (TED)}{11}}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\@writefile{toc}{\contentsline {paragraph}{Visual Information Fidelity (VIFD)}{12}}
\@writefile{tdo}{\contentsline {todo}{Explain why we chose 5 vs others. e.g. look at the Matlab implementation of this}{12}}
\pgfsyspdfmark {pgfid31}{20621296}{51313195}
\pgfsyspdfmark {pgfid34}{38844054}{51310902}
\pgfsyspdfmark {pgfid35}{40695445}{51092012}
\@writefile{toc}{\contentsline {paragraph}{The RRED Problem}{12}}
\@writefile{toc}{\contentsline {paragraph}{Testing these metrics}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.3}Classifier}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}2D based CNN Liveness Test}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1}Overview}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This is the output produced from the preprocessing step of the ResNet based method.\relax }}{13}}
\newlabel{FaceExtraction}{{6}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2}Data Preprocessing}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.3}The Model}{13}}
\@writefile{toc}{\contentsline {paragraph}{AlexNet}{13}}
\@writefile{toc}{\contentsline {paragraph}{Residual Networks}{13}}
\@writefile{toc}{\contentsline {paragraph}{Feed Forward Classifier}{13}}
\citation{liu_liu_2017}
\citation{liu_liu_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The 2D CNN test architecture. We take the face image, resize to a fixed size, and put through ResNet50. The two last CNN layers of this ResNet are trainable. The output of this network is flattened and fed through a deep feed forward network, yielding one output (which is the liveness score as before).\relax }}{14}}
\newlabel{2DCNNArchitecture}{{7}{14}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with Activation Functions}{14}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with a Single Neuron Output}{14}}
\@writefile{toc}{\contentsline {paragraph}{Normalization and Dropout}{14}}
\citation{SGDBetterThanAdamForImageClassification}
\citation{3DReconstructionMethod}
\citation{VRNTorchToKeras}
\citation{KerasVoxNetBug}
\citation{VoxNetModel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.4}The Training Process}{15}}
\@writefile{toc}{\contentsline {paragraph}{Optimizer}{15}}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{15}}
\@writefile{toc}{\contentsline {paragraph}{Data Generators}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}3D Face Reconstruction Liveness Test}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.1}Overview}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.2}2D Image Preprocessing}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.3}3D Facial Reconstruction}{15}}
\citation{3DMadDataset}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  This is an overview of the 3D classifier. \textbf  {(A)} a high resolution image is input into the classifier. \textbf  {(B)} The image goes through a HoG based face detector. The bounding box of the face is extracted, and the image is cropped. The image is then resized to be 192x192 pixels, which is what's required by the VRN process. \textbf  {(C)} The pretrained VRN face reconstruction model takes an image input, and outputs a voxel representation. Some preprocessing from the VRN model is necessary to convert an occupancy grid into a voxel representation (this is done here rather than in the VoxNet model). \textbf  {(D)} The VoxNet classifier uses several 3D convolutional layers, along with a couple of Dense layers to classify. \textbf  {(E)} The output of the last dense layer is simply a single number defined as the certainty of realness. 1.0 implies the model is certain that the input is real, while 0.0 implies the model is certain that the input is faked. \relax }}{16}}
\newlabel{3DClassifierArchitectureDiagram}{{8}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.4}3D Classification}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.5}The Training Process}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.6}The Dataset}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Consolidation Layer}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Outputs from the \emph  {live\_webcam\_output.py} file, being run on two different videos from the Replay-Attack test dataset. The model predicted the correct output here.\relax }}{17}}
\newlabel{SystemWorkingScreenshots}{{9}{17}}
\newlabel{RealScreenshot}{{9a}{17}}
\newlabel{sub@RealScreenshot}{{a}{17}}
\newlabel{FakeScreenshot}{{9b}{17}}
\newlabel{sub@FakeScreenshot}{{b}{17}}
\@writefile{tdo}{\contentsline {todo}{Implementation DETAIL NEEDED!}{17}}
\pgfsyspdfmark {pgfid36}{4474211}{33252458}
\pgfsyspdfmark {pgfid39}{38844054}{33250165}
\pgfsyspdfmark {pgfid40}{40695445}{33031275}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table of results, showing test accuracy with the percentage of test results falling into the specific category defined in the confusion matrix (obtained using sklearn).\relax }}{17}}
\newlabel{ResultsTable}{{1}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table of results, showing the wall clock time for the load and predict phases of both liveness tests.\relax }}{17}}
\newlabel{WallClockResultsTime}{{2}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Testing Process}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1}Datasets}{17}}
\citation{ImageQualityAssessmentTest}
\citation{VoxNetModel}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2}Accuracy}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3}Confusion Matrix}{18}}
\@writefile{toc}{\contentsline {paragraph}{True Positives}{18}}
\@writefile{toc}{\contentsline {paragraph}{False Positives}{18}}
\@writefile{toc}{\contentsline {paragraph}{True Negatives}{18}}
\@writefile{toc}{\contentsline {paragraph}{False Negatives}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Image Quality Liveness Test}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}2D Convolutional Neural Network Liveness Test}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}3D VoxNet Liveness Test}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A plot of ResNet output against WIQA output. As one can see, for two metrics a perceptron could be used due to the linearly separable nature of the data (that is, a single line could be used to separate the real and fake classifications in some cases). Effectively, with two metrics this would be acting as an OR gate (with fuzzy calculations).\relax }}{19}}
\newlabel{LinearlySeparableGraph}{{10}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Consolidation Layer}{19}}
\@writefile{tdo}{\contentsline {todo}{LDA: confusion matrix}{19}}
\pgfsyspdfmark {pgfid41}{35984220}{23890066}
\pgfsyspdfmark {pgfid44}{38844054}{23887773}
\pgfsyspdfmark {pgfid45}{40695445}{23668883}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Live Application}{19}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of CNN facial isolation}{19}}
\@writefile{toc}{\contentsline {paragraph}{Resolution problems with WIQA test}{19}}
\@writefile{toc}{\contentsline {paragraph}{Uncertainty of the CNN liveness test}{19}}
\@writefile{tdo}{\contentsline {todo}{Show uncertainty here somehow}{20}}
\pgfsyspdfmark {pgfid46}{30776584}{51313195}
\pgfsyspdfmark {pgfid49}{38844054}{51310902}
\pgfsyspdfmark {pgfid50}{40695445}{51092012}
\@writefile{toc}{\contentsline {paragraph}{Problem in running liveness tests in series}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Evaluation}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Usefulness of our system}{20}}
\@writefile{toc}{\contentsline {paragraph}{Quality Test}{20}}
\@writefile{toc}{\contentsline {paragraph}{2D CNN}{20}}
\@writefile{toc}{\contentsline {paragraph}{3D VoxNet Liveness Test}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Improvements}{20}}
\@writefile{toc}{\contentsline {paragraph}{Representing 3D Attacks}{20}}
\@writefile{toc}{\contentsline {paragraph}{Preventing Source-Quality Web Browser Attacks}{20}}
\@writefile{toc}{\contentsline {paragraph}{Existing BIQI implementation}{20}}
\bibstyle{abbrv}
\bibdata{report}
\bibcite{DeepNeuralNetworkDeployability}{{1}{}{{}}{{}}}
\bibcite{ReplayAttackDataset}{{2}{}{{}}{{}}}
\bibcite{SFMClassifier}{{3}{}{{}}{{}}}
\bibcite{VideoQualityUpdated}{{4}{}{{}}{{}}}
\bibcite{BIQIImplementation}{{5}{}{{}}{{}}}
\bibcite{3DMadDataset}{{6}{}{{}}{{}}}
\bibcite{ImageQualityAssessmentTest}{{7}{}{{}}{{}}}
\bibcite{LMSEPaper}{{8}{}{{}}{{}}}
\bibcite{DeepResidualNetworks}{{9}{}{{}}{{}}}
\bibcite{VideoQualityOriginal}{{10}{}{{}}{{}}}
\bibcite{3DReconstructionMethod}{{11}{}{{}}{{}}}
\bibcite{KerasVoxNetBug}{{12}{}{{}}{{}}}
\bibcite{NPRArticle}{{13}{}{{}}{{}}}
\bibcite{AlexNet}{{14}{}{{}}{{}}}
\bibcite{FaceSpoofingAttacksStudy}{{15}{}{{}}{{}}}
\bibcite{liu_liu_2017}{{16}{}{{}}{{}}}
\bibcite{VRNTorchToKeras}{{17}{}{{}}{{}}}
\bibcite{VoxNetModel}{{18}{}{{}}{{}}}
\bibcite{BlinkDetectionLivenessTest}{{19}{}{{}}{{}}}
\bibcite{Patel2016CrossDatabaseFA}{{20}{}{{}}{{}}}
\bibcite{PointNet}{{21}{}{{}}{{}}}
\bibcite{GoogLeNet}{{22}{}{{}}{{}}}
\bibcite{NUAADataset}{{23}{}{{}}{{}}}
\bibcite{JQIPaper}{{24}{}{{}}{{}}}
\bibcite{SSIMPaper}{{25}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{21}}
\bibcite{SGDBetterThanAdamForImageClassification}{{26}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
