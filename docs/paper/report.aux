\relax 
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{report.ist}
\@glsorder{word}
\@writefile{toc}{\contentsline {paragraph}{Context}{1}}
\@writefile{toc}{\contentsline {paragraph}{Aims}{1}}
\@writefile{toc}{\contentsline {paragraph}{Method}{1}}
\@writefile{toc}{\contentsline {paragraph}{Results}{1}}
\citation{NPRArticle}
\citation{FaceSpoofingAttacksStudy}
\citation{FaceSpoofingAttacksStudy}
\@writefile{toc}{\contentsline {paragraph}{Conclusions}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}2D Spoofing Attacks}{2}}
\citation{SFMClassifier}
\citation{SFMClassifier}
\citation{BlinkDetectionLivenessTest}
\citation{ImageQualityAssessmentTest}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1}Video-based tests}{3}}
\@writefile{toc}{\contentsline {paragraph}{Depth-based liveness}{3}}
\@writefile{toc}{\contentsline {paragraph}{Blink Detection}{3}}
\@writefile{toc}{\contentsline {paragraph}{Face Flashing}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2}Quality-based tests}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3}Neural Networks, and their structure}{3}}
\@writefile{toc}{\contentsline {paragraph}{Fully connected layers}{3}}
\citation{Patel2016CrossDatabaseFA}
\citation{AlexNet}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{GoogLeNet}
\citation{DeepResidualNetworks}
\citation{DeepResidualNetworks}
\@writefile{toc}{\contentsline {paragraph}{Convolutional layers}{4}}
\@writefile{toc}{\contentsline {paragraph}{A note about parameters}{4}}
\@writefile{toc}{\contentsline {paragraph}{Cross-Database Approach}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.4}General 2D image classification models}{4}}
\@writefile{toc}{\contentsline {paragraph}{AlexNet}{4}}
\@writefile{toc}{\contentsline {paragraph}{VGG16 Network}{4}}
\@writefile{toc}{\contentsline {paragraph}{GoogLeNet Inception}{4}}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{DeepNeuralNetworkDeployability}
\citation{NUAADataset}
\citation{ReplayAttackDataset}
\newlabel{Top1AccuracyOverNetwork}{{2a}{5}}
\newlabel{sub@Top1AccuracyOverNetwork}{{a}{5}}
\newlabel{Top1AccuracyOverOperations}{{2b}{5}}
\newlabel{sub@Top1AccuracyOverOperations}{{b}{5}}
\newlabel{ComparisonsOfDifferentModels}{{\caption@xref {ComparisonsOfDifferentModels}{ on input line 253}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces These two charts, from \cite  {DeepNeuralNetworkDeployability}, show the accuracy and computational requirements of each type of classifier.\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A Residual Block: this is the building block of the ResNet architecture. Source: \cite  {DeepResidualNetworks}\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{ResidualBlock}{{1}{5}}
\@writefile{toc}{\contentsline {paragraph}{Residual Networks}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.5}Datasets}{5}}
\citation{FaceSpoofingAttacksStudy}
\citation{3DMadDataset}
\citation{SFMClassifier}
\citation{3DReconstructionMethod}
\citation{VRNTorchToKeras}
\citation{VRNTorchToKeras}
\citation{PointNet}
\citation{VoxNetModel}
\citation{VoxNetModel}
\citation{VoxNetModel}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}3D Spoofing Attacks}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Dataset}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1}Obtaining 3D data from 2D images/video}{6}}
\@writefile{toc}{\contentsline {paragraph}{Structure from Motion (SFM)}{6}}
\@writefile{toc}{\contentsline {paragraph}{Single Image 3D Face Reconstruction (VRN)}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A screenshot from the VRN Torch to Keras version (a conversion of the VRN model to Keras). This screenshot is from \cite  {VRNTorchToKeras}\relax }}{6}}
\newlabel{3DReconstructionScreenshot}{{3}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2}3D Classifiers}{6}}
\@writefile{toc}{\contentsline {paragraph}{PointNet}{6}}
\@writefile{toc}{\contentsline {paragraph}{VoxNet}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The original VoxNet architecture. While point cloud input is available, voxel based representation can also be used. Diagram obtained from \cite  {VoxNetModel}\relax }}{7}}
\newlabel{OriginalVoxNetArchitecture}{{4}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Solution}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Specification, and design requirements}{7}}
\newlabel{SpecPoint1}{{1}{7}}
\newlabel{SpecPoint2}{{2}{7}}
\newlabel{SpecPoint3}{{3}{7}}
\newlabel{SpecPoint4}{{4}{7}}
\newlabel{SpecPoint5}{{5}{7}}
\newlabel{SpecPoint6}{{6}{7}}
\newlabel{SpecPoint7}{{7}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Shared Services}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1}Dataset Managers}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.2}Neural Network Infrastructure}{8}}
\@writefile{toc}{\contentsline {paragraph}{Neural Network Framework}{8}}
\@writefile{toc}{\contentsline {paragraph}{Hardware for training}{8}}
\@writefile{toc}{\contentsline {paragraph}{Hardware for testing}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3}Image Processing and Computation Management}{8}}
\citation{ImageQualityAssessmentTest}
\citation{VideoQualityOriginal}
\citation{VideoQualityUpdated}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The architecture of the image quality liveness test. (A) The greyscale copy of the image, and a blurred copy of the image are input into each of the metrics. (B) The metrics are individually calculated, and a single value output from them. (C) These values build a 1D vector. (D) They are classified using an LDA classifier. (E) The realness value is 1.0 for real, and 0.0 for fake, or in between.\relax }}{9}}
\newlabel{ImageQualityLivenessTestDiagram}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Image Quality Assessment Liveness Test}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1}Overview}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2}The Metrics}{9}}
\citation{BIQIImplementation}
\@writefile{toc}{\contentsline {paragraph}{PyVideoQuality Library}{10}}
\@writefile{toc}{\contentsline {paragraph}{Blind Image Quality Index (BIQI)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Magnitude Error (GME)}{10}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Phase Error (GPE)}{10}}
\@writefile{toc}{\contentsline {paragraph}{High Low Frequency Index (HLFI)}{10}}
\@writefile{toc}{\contentsline {paragraph}{JPEG Quality Index (JQI)}{10}}
\citation{JQIPaper}
\citation{LMSEPaper}
\citation{LMSEPaper}
\citation{LMSEPaper}
\citation{LMSEPaper}
\@writefile{toc}{\contentsline {paragraph}{Laplacian Mean Squared Metric (LMSE)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Mean Angle Magnitude Similarity (MAMS)}{11}}
\@writefile{toc}{\contentsline {paragraph}{Mean Angle Similarity (MAS)}{11}}
\@writefile{toc}{\contentsline {paragraph}{R-Averaged Metric (RAMD)}{12}}
\@writefile{toc}{\contentsline {paragraph}{Spectral Magnitude Error (SME)}{12}}
\@writefile{toc}{\contentsline {paragraph}{Spectral Phase Error (SPE)}{12}}
\@writefile{toc}{\contentsline {paragraph}{Total Corner Difference (TCD)}{12}}
\@writefile{toc}{\contentsline {paragraph}{Total Edge Difference (TED)}{12}}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\citation{ImageQualityAssessmentTest}
\@writefile{toc}{\contentsline {paragraph}{Visual Information Fidelity (VIFD)}{13}}
\@writefile{toc}{\contentsline {paragraph}{The other metrics used}{13}}
\@writefile{toc}{\contentsline {paragraph}{The RRED Problem}{13}}
\@writefile{toc}{\contentsline {paragraph}{Testing these metrics}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.3}Classifier}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}2D based CNN Liveness Test}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1}Overview}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2}Data Preprocessing}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces This is the output produced from the preprocessing step of the ResNet based method.\relax }}{14}}
\newlabel{FaceExtraction}{{6}{14}}
\citation{liu_liu_2017}
\citation{liu_liu_2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.3}The Model}{15}}
\@writefile{toc}{\contentsline {paragraph}{Early attempts with AlexNet}{15}}
\@writefile{toc}{\contentsline {paragraph}{Residual Networks}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The 2D CNN test architecture. We take the face image, resize to a fixed size, and put through ResNet50. The two last CNN layers of this ResNet are trainable. The output of this network is flattened and fed through a deep feed forward network, yielding one output (which is the liveness score as before).\relax }}{15}}
\newlabel{2DCNNArchitecture}{{7}{15}}
\@writefile{toc}{\contentsline {paragraph}{Feed Forward Classifier}{15}}
\@writefile{toc}{\contentsline {paragraph}{The Problem with Activation Functions}{15}}
\citation{SGDBetterThanAdamForImageClassification}
\@writefile{toc}{\contentsline {paragraph}{The Problem with a Single Neuron Output}{16}}
\@writefile{toc}{\contentsline {paragraph}{Normalization and Dropout}{16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.4}The Training Process}{16}}
\@writefile{toc}{\contentsline {paragraph}{Optimizer}{16}}
\@writefile{toc}{\contentsline {paragraph}{Loss Function}{16}}
\@writefile{toc}{\contentsline {paragraph}{Data Generators}{16}}
\citation{3DReconstructionMethod}
\citation{VRNTorchToKeras}
\citation{KerasVoxNetBug}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}3D Face Reconstruction Liveness Test}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.1}Overview}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.2}Preprocessing Part 1: Extracting Facial Data}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.3}3D Facial Reconstruction}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  This is an overview of the 3D classifier. \textbf  {(A)} a high resolution image is input into the classifier \textbf  {(B)} Preprocessing (facial extraction, and resize to 192x192) \textbf  {(C)} 2D to 3D reconstruction \textbf  {(D)} Conv3D layers along with feed-forward layers, used for learning the classification \textbf  {(E)} A single realness metric is output, 1.0 implies real, while 0.0 implies spoofing has occurred. \relax }}{17}}
\newlabel{3DClassifierArchitectureDiagram}{{8}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.4}3D Classification}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.5}The Training Process}{17}}
\citation{3DMadDataset}
\citation{ImageQualityAssessmentTest}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {E.6}The Dataset}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Consolidation Layer}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Table of results, showing test accuracy with the percentage of test results falling into the specific category defined in the confusion matrix (obtained using sklearn).\relax }}{18}}
\newlabel{ResultsTable}{{1}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Table of results, showing the wall clock time for the load and predict phases of both liveness tests.\relax }}{18}}
\newlabel{WallClockResultsTime}{{2}{18}}
\citation{ImageQualityAssessmentTest}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Outputs from the \emph  {live\_webcam\_output.py} file, being run on two different videos from the Replay-Attack test dataset. The model predicted the correct output here.\relax }}{19}}
\newlabel{SystemWorkingScreenshots}{{9}{19}}
\newlabel{RealScreenshot}{{9a}{19}}
\newlabel{sub@RealScreenshot}{{a}{19}}
\newlabel{FakeScreenshot}{{9b}{19}}
\newlabel{sub@FakeScreenshot}{{b}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Testing Process}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1}Datasets}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2}Accuracy}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3}Confusion Matrix}{19}}
\@writefile{toc}{\contentsline {paragraph}{True Positives}{19}}
\@writefile{toc}{\contentsline {paragraph}{False Positives}{19}}
\@writefile{toc}{\contentsline {paragraph}{True Negatives}{19}}
\@writefile{toc}{\contentsline {paragraph}{False Negatives}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Image Quality Liveness Test}{19}}
\citation{VoxNetModel}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}2D Convolutional Neural Network Liveness Test}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}3D VoxNet Liveness Test}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Consolidation Layer}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A plot of ResNet output against WIQA output. While a perceptron could be used (due to the linearly separable nature), this would effectively yield a logical AND or OR gate with 2 liveness tests.\relax }}{20}}
\newlabel{LinearlySeparableGraph}{{10}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Live Application}{21}}
\@writefile{toc}{\contentsline {paragraph}{Benefits of CNN facial isolation}{21}}
\@writefile{toc}{\contentsline {paragraph}{Resolution problems with WIQA test}{21}}
\@writefile{toc}{\contentsline {paragraph}{Uncertainty of the CNN liveness test}{21}}
\@writefile{toc}{\contentsline {paragraph}{Problem in running liveness tests in series}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Evaluation}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Usefulness of our system}{21}}
\@writefile{toc}{\contentsline {paragraph}{Quality Test}{21}}
\bibstyle{abbrv}
\bibdata{report}
\bibcite{DeepNeuralNetworkDeployability}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {paragraph}{2D CNN}{22}}
\@writefile{toc}{\contentsline {paragraph}{3D VoxNet Liveness Test}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Improvements}{22}}
\@writefile{toc}{\contentsline {paragraph}{Representing 3D Attacks}{22}}
\@writefile{toc}{\contentsline {paragraph}{More Liveness Tests}{22}}
\@writefile{toc}{\contentsline {paragraph}{Existing BIQI implementation}{22}}
\@writefile{toc}{\contentsline {paragraph}{Replacing ResNet50 with a larger ResNet model}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusions}{22}}
\bibcite{ReplayAttackDataset}{{2}{}{{}}{{}}}
\bibcite{SFMClassifier}{{3}{}{{}}{{}}}
\bibcite{VideoQualityUpdated}{{4}{}{{}}{{}}}
\bibcite{BIQIImplementation}{{5}{}{{}}{{}}}
\bibcite{3DMadDataset}{{6}{}{{}}{{}}}
\bibcite{ImageQualityAssessmentTest}{{7}{}{{}}{{}}}
\bibcite{LMSEPaper}{{8}{}{{}}{{}}}
\bibcite{DeepResidualNetworks}{{9}{}{{}}{{}}}
\bibcite{VideoQualityOriginal}{{10}{}{{}}{{}}}
\bibcite{3DReconstructionMethod}{{11}{}{{}}{{}}}
\bibcite{KerasVoxNetBug}{{12}{}{{}}{{}}}
\bibcite{NPRArticle}{{13}{}{{}}{{}}}
\bibcite{AlexNet}{{14}{}{{}}{{}}}
\bibcite{FaceSpoofingAttacksStudy}{{15}{}{{}}{{}}}
\bibcite{liu_liu_2017}{{16}{}{{}}{{}}}
\bibcite{VRNTorchToKeras}{{17}{}{{}}{{}}}
\bibcite{VoxNetModel}{{18}{}{{}}{{}}}
\bibcite{BlinkDetectionLivenessTest}{{19}{}{{}}{{}}}
\bibcite{Patel2016CrossDatabaseFA}{{20}{}{{}}{{}}}
\bibcite{PointNet}{{21}{}{{}}{{}}}
\bibcite{GoogLeNet}{{22}{}{{}}{{}}}
\bibcite{NUAADataset}{{23}{}{{}}{{}}}
\bibcite{JQIPaper}{{24}{}{{}}{{}}}
\bibcite{SGDBetterThanAdamForImageClassification}{{25}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
